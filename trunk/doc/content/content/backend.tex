\chapter{Backend}

Bevor wir zur Besprechung der Umsetzung des eigentlichen Backends übergehen, wollen wir zuerst einige grundsätzliche Überlegungen zur Art des Prozesses, den wir für unser Backend einsetzen wollen, festhalten.

\section{Art des Prozesses}\label{lbl_prozess_art}

\subsection{Prozesse unter Android}\label{lbl_be_procandroid}

Da in der Entwicklung für das Android-Betriebssystem ein etwas erweitertes Vokabular für die Beschreibung des Applikations-Lebenszyklus verwendet wird, wollen wir zuerst kurz auf einige dieser Begriffe im Android-Kontext eingehen.

Applikationen laufen unter Android grundsätzlich in einem eigenen Thread den das Betriebssystem als separaten Linux-Prozess startet und werden auch vom Betriebssystem wieder beendet. Sollen Komponenten in einem eigenen Prozess laufen, können weitere Threads gestartet werden\cite{adglc}. Diese können über die standard Thread-Objekte von Java erzeugt werden, werden aber mit dem Applikations-Prozess terminiert, sollte Android entscheiden dass dieser nicht mehr benötigt wird. Um dies, wenn Ressourcen benötigt werden sollten, möglich effizient durchzuführen, werden Prozesse vom Betriebssystem einfach abgewürgt (ein kill-Signal wird gesendet) und keinerlei Aufräumfunktionen werden ausgeführt. Applikationen sind daher selbst dafür verantwortlich, ihren Status zu sichern, wenn sie in den Hintergrund geschickt werden\cite{adbmt}.

Unser Erkennungs-Backend sollte nun aber auch unabhängig vom Frontend weiterlaufen. Grundsätzlich wäre das beenden des Backend-Threads beim Schliessen des Frontends kein grösseres Problem, da wir davon ausgehen können, dass eventuell erst nach Beendung des Frontend-Threads eintreffende Resultate für den Benutzer nicht mehr relevant sind und verworfen werden können bzw. nicht mehr fertig gerechnet werden müssen. Allerdings wäre es zu bevorzugen, wenn bei einem erneuten Aufruf des Frontends der Backend-Thread schon im Speicher vorhanden ist und nicht noch neu initialisiert werden muss. Die Initialisierung des Backends kann je nach verwendeter Erkennungs-Methodik recht aufwendig sein. So muss etwa bei der Graphen-Methode zuerst die XML-Datei, in welcher der Graph gespeichert ist, eingelesen und der Graph aufgebaut werden. Der Benutzer erwartet möglichst schnell mit der Eingabe beginnen zu können und eine Initialisierung bei jeder Versuch dazu sollte ihm daher nicht zugemutet werden.

Neben den normalen Threads aus Java kennt Android noch spezialisierte Prozesse, die unabhängig von der Ursprungs-Applikation weiterlaufen können. Für uns interessant sind hier vor allem die Services und die Broadcast-Receiver.

\subsubsection{Broadcast-Receiver}

Broadcast-Receiver werden hauptsächlich dazu verwendet, im Hintergrund auf Ereignisse zu reagieren. Die Handhabung diese Ereignisses durch den Receiver muss innerhalb einer gegebenen Zeit erfolgen\footnote{Im Moment beträgt die zur Verfügung stehende Zeit 10 Sekunden, danach kann der Prozess jederzeit abgewürgt werden\cite{adbmt}}, wodurch diese nur im Speicher gehalten werden müssen, wenn ein solches Ereignis eintrifft. Broadcast-Receiver sind daher also gut geeignet um kurze Arbeiten als Reaktion auf eine externen Stimulus durchzuführen und werden etwa benutzt um auf Alarme oder Positions-Änderungen zu reagieren. Da sie aber nach Beendigung ihrer Aufgabe voraussichtlich wieder terminiert werden, würden sie für unseren Backend-Prozess keinen Vorteil darstellen, da sich die Frage der Reinitialisierung ebenfalls wieder stellen würde.

\subsubsection{Services}

Services sind unter Android sehr vielseitig und durch sie können Applikationen langlaufende Operationen im Hintergrund ausführen. Grundsätzlich ist ihre Laufzeit nicht beschränkt und ihr Verhalten in dieser Zeit wird weitestgehend von der Applikation bestimmt. So können sie etwa vom restlichen Programm unabhängige Operationen ausführen, auf gemeinsame Singelton-Objekte zugreifen oder gar in einen eigenen Prozess ausgelagert werden.

Services können allerdings vom Betriebssystem jederzeit abgeschossen werden, wenn es Ressourcen benötigen sollte. Allerdings wird sich Android merken, wenn es Services, die weiter laufen wollen, terminiert. Wenn wieder genug Arbeitsspeicher verfügbar ist, werden die beendeten Services wieder neu gestartet. Auch ist es möglich, dass Services beantragen, als Vordergrund-Prozesse behandelt zu werden und so recht gut vor dem Terminieren geschützt sind. Dies muss dem Benutzer allerdings angezeigt werden, damit sich dieser des laufenden Prozesses bewusst ist.

Für unser Backend sollte ein Service also die am besten geeignete Prozess-Form sein, da wir den Backend-Service im Hintergrund weiter laufen lassen können, nachdem eine Eingabe getätigt wurde. Eine Terminierung bei zu wenig vorhandenem Speicher können wir akzeptieren, da der Service danach wieder neu gestartet werden sollte und die Initialisierung vom Benutzer unbemerkt im Hintergrund stattfinden kann. Wir benötigen daher auch nicht die Privilegien eines Vordergrund-Prozesses.
%================================================

\section{Analyse}

\subsection{Initialisierung}

\subsection{Ablauf der Erkennungs}

Um den Ablauf der Erkennung möglichst flexibel zu machen, wurde dieser in mehrere Phasen aufgeteilt. Einige dieser Phasen sollen wiederum in mehrere Einzelschritte aufgeteilt werden.

\begin{figure}[h]
   \centering
   \includegraphics[width=\textwidth]{img/erkennungs_ablauf.pdf} 
   \caption{Erkennungs-Ablauf}
   \label{fig:erkennungs_ablauf}
\end{figure}

Zuerst soll eine Vorverarbeitung der Eingabe-Punkte durchgeführt werden, in der etwa eine Normierung, eine Glättung, eine Filterung oder ähnliches dieser durchgeführt werden kann. Diese Verarbeitungs-Schritte sollen sich nicht gegenseitig ausschliessen, sollen aber auch optional sein, da einige der dazu möglichen Algorithmen sehr rechenaufwendig sind.

Die zweite Phase soll die Erkennung der Mikrogesten sein. Auf diese soll in mehrere Erkennungs-Schritte aufgeteilt sein, die gegebenenfalls auch deaktiviert werden können.

Als nächstes soll die Zeichen-Erkennung durchgeführt werden. Diese soll allerdings in sich geschlossen funktionieren und nicht zwingend weiter in Teil-Schritte aufgeteilt werden. Das Erkennungs-Verfahren als ganzes soll allerdings austauschbar sein.

Die vierte und letzte Phase soll die Nachverarbeitung der erkannten Zeichen sein. Dies kann etwa bedeuten, das gewisse Zeichen wieder verworfen werden oder das die Gewichtung der Erkennungs-Wahrscheinlichkeit der Zeichen angepasst wird.

\subsubsection{Vorverarbeitung}

Häufig könnte die Erkennungsgenauigkeit der Mikrogesten-Erkennung durch eine passende Vorverarbeitung der Eingabe-Punkte erheblich verbessert werden. Allerdings sind viele der geeigneten Algorithmen sehr rechenaufwendig und können die Geschwindigkeit der Erkennung selbst auf leistungsfähigen Systemen erheblich verlangsamen\footnote{Unsere Test-Hardware, das Nexus One von Google mit einem 1 GHz Snapdragon Prozessor, benötigte bei einigen Algorithmen etwa mehrere Sekunden für die Glättung der Eingabe-Punkte normaler Buchstaben}. Daher war es unser Ziel, den Einsatz solche Vorverarbeitungs-Algorithmen zwar zu ermöglichen, diese allerdings optional zu machen. Insbesondere soll es möglich machen, dass diese in einer späteren Version auch aus dem Frontend heraus zu aktivieren oder deaktivieren.

Auch wollten wir eine serielle Abarbeitung von mehreren Vorverarbeitungs-Schritten möglich zu machen. Dabei muss es möglich sein, diese in einer festlegbaren Reihenfolge ausgeführt werden. So könnte zum Beispiel gewünscht werden, dass zuerst gewisse Punkte heraus gefiltert werden, bevor eine Glättung durchgeführt wird, um den Rechenaufwand für diese zu reduzieren.

\subsubsection{Mikrogesten-Erkennung}

Auch bei der Erkennung der Mikrogesten soll es möglich sein, diese in mehrere Schritte aufzuteilen und diese in einer definierbaren Reihenfolge durchzuführen. Dies kann etwa zum Testen verschiedener Ansätze nützlich sein, aber auch zum Feintuning eines speziellen Ansatzes. Ein Beispiel wäre hierbei, dass zuerst die harten Kanten im Pfad der Eingabe-Punkte detektiert werden und danach die einzelnen Mikrogesten in diesen Kanten gesucht werden.

Auch hier soll eine Anpassung zur Laufzeit durch das Frontend grundsätzlich möglich sein und von unserem Design unterstützt werden.

\subsubsection{Zeichen-Erkennung}

Auch das Austauschen des Algorithmus zur Zeichen-Erkennung zur Laufzeit soll grundsätzlich möglich sein. Allerdings soll es nur möglich sein, diesen als Ganzes auszutauschen. Eine Unterteilung in Teilschritte soll nicht von der Architektur her vorgeschrieben werden und eine Umorganisation dieser während der Laufzeit ist nicht angedacht, da wir die eigentliche Zeichen-Erkennung als monolithischen Vorgang gestalten wollen.

So können verschiedene Ansätze zur Zeichen-Erkennung anhand der Mikrogesten miteinander verglichen und alternative Erkennungs-Strategien angeboten werden. Neben dem in dieser Arbeit verwendeten Ansatz für eine Erkennung durch einen Graphen, ist etwa eine Erkennung über ein neurales Netz eine naheliegende Möglichkeit.

Speziell für eine Erkennungs-Ansatz über ein neurales Netz, aber auch für den Graphen-Ansatz, wäre eine Möglichkeit zur Anpassung der zur Erkennung verwendeten Datenbasis von Vorteil. Daher wollen wir auch eine Möglichkeit zur Nachverarbeitung der Daten bieten.

\subsubsection{Nachverarbeitung}

Für die Nachverarbeitung wollen wir einen zur Zeichen-Erkennung analogen Ansatz verwenden. Das heisst dass es möglich sein soll, den verwendete Algorithmus auszutauschen, eine Unterteilung in Teilschritte soll aber nicht gefordert werden. Im Gegensatz zur Zeichen-Erkennung soll die Nachverarbeitung aber optional sein.

Mögliche Einsatz-Zwecke für Nachverarbeitungs-Algorithmen wäre etwa, wie bereits erwähnt, die Anpassung der vorhandenen Datenbasis an den individuellen Benutzer in einem Trainingsmodus, die weitere Einschränkung oder Anpassung der Erkennungs-Wahrscheinlichkeiten der erkannten Zeichen, etwa anhand eines Wörterbuches.
%================================================

\section{Design}

\subsection{Schnittstellen}

Wie bereits in \ref{lbl_prozess_art} besprochen, wollen wir für das Backend einen Service definieren. Damit dieser auch wirklich unabhängig (also in einem eigenen Prozess) vom Frontend läuft, soll dieser ein Remote-Service sein. Dies bedeutet, dass wir für die Kommunikation zwischen dem Frontend und dem Backend Schnittstellen definieren müssen, um die Inter-Prozess-Kommunikation abzuwickeln. Android stellt dafür eine eigenen Dialekt der Interface Definition Language bereit, der sich wenig überraschend "Android Interface Definition Language" (kurz AIDL) nennt.\\
\\
Die Schnittstellen, die wir benötigen werden, lassen sich in zwei Gruppen unterteilen: Die eigentliche Zeichen-Erkennung und die Verwaltung der Erkennungs-Algorithmen und deren Konfiguration.\\

\begin{lstlisting} [caption={AIDL-Definitionen der Service-Schnittstelle},label=intrf_detection]
interface IDetectionService {
  //...

  void addTouchPoints(
    in List<TouchPoint> points, boolean start_detection);
  //...
  void endSample();

  List<String> getAvailableStrategies();
  List<String> getActiveStrategies();
  void reinitializeStrategies(int type);
  List<StrategyArgument> getStrategyConfiguration(
    String strategy_name, int type);
  void setStrategyArgument(
    in StrategyArgument argument, int type);
  void broadcastArgument(in StrategyArgument argument);
}
\end{lstlisting}
\newpage
\begin{lstlisting} [caption={AIDL-Definitionen der R\"{u}ckruf-Schnittstelle},label=intrf_detection]
oneway interface IReturnResults {
  void recognisedCharacters(
    in List<Character> characters);
  //...
}
\end{lstlisting}

\subsubsection{Zeichen-Erkennung}

Die Schnittstellen für die Zeichen-Erkennung sind relativ simpel und direkt: Es müssen Eingabe-Punkte an das Backend gesendet werden und es müssen die erkannten Zeichen an das Frontend zurück gegeben werden. Da die Rückgabe der erkannten Zeichen nicht zwingend unmittelbar nach der Übertragung der Eingabe-Punkte erfolgen muss, wird sie über ein eigenes Rückruf-Interface abgehandelt.

Das Interface \emph{IDetectionService} definiert dafür unter anderem die Methoden \emph{addTouchPoints()}, mit welcher neue Eingabe-Punkte an den Erkennungs-Dienst übermittelt und die Erkennung falls gewünscht gestartet werden kann, sowie \emph{endSample()}, welche eine Eingabe abschliesst und die Erkennung started.

Für die Rückgabe definiert das Rückruf-Interface \emph{IReturnResults} unter anderem die Methode \emph{recognisedCharacters()}, mit welcher die erkannten Zeichen an das Frontend zurück gegeben werden.

\subsubsection{Verwaltung und Konfiguration der Erkennungs-Algorithmen}

Für die Verwaltung der Erkennungs-Algorithmen stellt das Interface \emph{IDetectionService} zum Einen Methoden für das Auslesen der Verwendeten Algorithmen und zum Anderen für solche für die Konfiguration der einzelnen Algorithmen bereit.

Die Methode \emph{getAvailableStrategies()} gibt alle im Service vorhandenen Algorithmen zurück, während \emph{getActiveStrategies()} nur die momentan aktiven liefert. Ein solcher Algorithmus kann über die Konfigurations-Schnittstellen aktiviert oder deaktiviert werden. Weiter können mit der Methode \emph{reinitializeStrategies()} alle Algorithmen einer bestimmten Phase\footnote{Wird als Type der Phase ein Wert unter Null angegeben, werden alle Algorithmen neu initialisiert.} neu initialisiert werden werden.

\label{lbl_be_enable}Für die Konfiguration der Algorithmen wurde die Klasse \emph{StrategyArgument} entworfen, welche das Interface \emph{Parcelable} des Android-Frameworks implementiert und daher über die Kommunikations-Mechanismen von Android versandt werden kann. Die Klasse repräsentiert ein Argument eines Algorithmus und enthält dessen Namen, den Namen des Argumentes, sowie dessen Wert und Beschreibung. Ein Spezialfall eines solchen Arguments ist die Aktivierung oder Deaktivierung eines Algorithmus. Mit einem \emph{StrategyArgument} mit dem Namen "enabled" und dem Wert "false" kann ein Algorithmus deaktiviert werden, alle anderen Wert aktivieren ihn. Mit der Methode \emph{getStrategyConfiguration()} können alle Argumente eines Algorithmus ausgelesen werden. Die Methode \emph{setStrategyConfiguration()} setzt den Wert des übergebenen Arguments in dem im \emph{StrategyArgument} spezifizierten Algorithmus. Weiter ist auch noch die Methode \emph{broadcastArgument()} definiert, welche den Wert des übergebenen Argumentes in allen Algorithmen setzt, welche dieses unterstützen. Diese Methode wird etwa dafür verwendet, die Höhe des jeweiligen Eingabe-Feldes an alle interessierten Algorithmen zu melden.

\subsection{Erkennungs-Alogrithmen}

Da die einzelnen Schritte unseres Erkennungs-Ablaufs innerhalb der jeweiligen Phasen die selben Eingabe- und Ausgabe-Typen verwenden und dort austauschbar sein sollen lag die Anwendung des Strategy Design Patterns\cite[S.315-323]{designpatterns} für diese nahe. Jeder dieser Algorithmen ist somit eine Instanz einer der Phase entsprechenden Strategie.

\subsubsection{Strategien}

Da die Strategien aller Phasen ausser den Typen der Ein- und Rückgabewerte der Methode, die ihren eigentlichen Algorithmus implementiert, sehr ähnliche Attribute und Methoden haben, wurde ein globales Interface für die Strategien namens \emph{IStrategy} definiert, sowie eine abstrakte Basis-Klasse namens \emph{BaseStrategy} entworfen. Diese übernimmt etwa die Verwaltung der Argumente einer Strategie. Dadurch müssen die konkreten Implementationen von \emph{IStrategy}\footnote{Beziehungsweise der davon abgeleiteten Interfaces für die einzelnen Phasen.} nur den eigentlichen Algorithmus sowie ein paar einfache Methoden zur Beschreibung der Strategie umfassen. Dies erlaubt uns, Quelltext-Redundanzen zu minimieren und vereinfacht das Hinzufügen von neuen Algorithmen erheblich.

\subsubsection{Verwaltung der Strategien}

Alle bekannten Strategien einer Phase werden zur einfachen Auffindbarkeit und Verwaltung durch entsprechenden Manager-Klassen verwaltet. Auf für die Manager der einzelnen Phasen wurde wieder eine verallgemeinerte abstrakte Basis-Klasse entworfen, welche die eigentlichen Verwaltungs-Operationen implementiert. Für die Implementation der Phasen-Manager wurden diese dann nur noch von der Basis-Klasse namens \emph{StrategyManager} abgeleitet und die einzelnen Strategien mussten noch initialisiert und dem Manager hinzugefügt werden.

\begin{figure}[h]
   \centering
   \includegraphics[width=\textwidth]{img/uml_cd_manager} 
   \caption{Klassen-Diagramm der Manager}
   \label{fig:cl_manager}
\end{figure}

\subsection{Erkennungs-Service}

Da der Erkennungs-Dienst als Service in einem unabhängigen Hintergrund-Prozess arbeiten soll, wird dieser natürlich von der Service-Klasse von Android abgeleitet und muss entsprechend die verlangten Methoden implementieren (siehe \ref{lbl_be_impl_service}).

\begin{figure}[h]
   \centering
   \includegraphics[width=\textwidth]{img/uml_cd_strategies} 
   \caption{Klassen-Diagramm der Strategien}
   \label{fig:cl_strategies}
\end{figure}

Weiter muss der Service den Ablauf seiner Erkennung verwalten. Dazu soll er für die beiden mehrstufigen Phasen jeweils eine Priority-Queue erhalten in welcher die einzelnen Erkennungs-Schritte anhand ihrer Position im Erkennungs-Prozess priorisiert abgelegt werden sollen.

\begin{figure}[h]
   \centering
   \includegraphics[width=\textwidth]{img/uml_cd_queue} 
   \caption{Klassen-Diagramm der Priority-Queues}
   \label{fig:cl_queue}
\end{figure}

Während der Erkennung muss der \emph{DetectionService} nun für jede Phase durch diese jeweilige \emph{StrategyQueue} iterieren und die aktiven Strategien, beziehungsweise bei einstufigen Phasen nur die entsprechende Strategie, anwenden. Zwischen den einzelnen Phasen muss der Dienst jeweils noch die Eingabe-Punkte oder die Resultate der vorgängigen Phase in das Eingabe-Format der nächsten Stufe konvertieren. Gegebenenfalls müssen vor dem Start der Erkennung noch die Eingabe-Punkte aus einem Buffer in die Liste der Eingabe-Punkte verschoben werden.

Nachdem alle Erkennungs-Phasen durchlaufen wurden werden die Resultate dann über das Rückruf-Interface an alle angebunden Clients versendet.

(Sequenz-Diagramm DetectionService.startDetection())

%================================================

\section{Implementation}

In diesem Abschnitt sollen nur die interessanteren Aspekte der Implementation erläutert werden und die Erweiterung des Systems mit zusätzlichen Funktionen beschrieben werden. Für Details der Implementation soll hiermit auf die API-Dokumentation\footnote{Zu finden in Form von mit Javadoc und Doxygen generiertem HTML im Ordner ''doc'' unter dem Projektverzeichnis} und den Quellcode verwiesen werden.

\subsection{Strategien}

\subsubsection{Zeichen-Erkennungsstrategie Graph}

Um die Erkennung der Zeichen anhand der zuvor erkannten Mikrogesten mit Hilfe eines Graphen durchführen zu können, muss dieser zuvor initialisiert werden. Da wir uns eine gewisse Flexibilität bei der Definition des Graphen erhalten wollte, wurde entschieden, dessen Daten im standardisierten, XML-basierenden GraphML-Format\cite{graphml_primer} abzuspeichern und diese jeweils beim Start des \emph{DetectionService} einzulesen. Wie in \ref{lbl_be_procandroid} bereits erläutert, sollen die Daten des Graphen während der Laufzeit im Speicher gehalten werden, im die Verzögerung für den Anwender beim Aufstarten der Anwendung möglichst gering zu halten. Trotzdem soll es möglich sein, die mit der Applikation mitgelieferte GraphML-Datei durch eine eigene zu ersetzen, sollte der Benutzer dies wünschen oder ein Frontend dies umsetzen wollen.

Daher besitzt die Erkennungs-Strategie \emph{CharacterDetectionStrategyGraph} ein Argument das denn Ort an dem die GraphML-Datei gespeichert ist spezifizieren kann. Der Einfachheit halber wird neben dem Laden der mitgelieferten Datei aus den Applikations-Ressourcen nur das Laden von einer SD-Karte unterstützt. Der Benutzter oder ein Frontend müssten ihre eigene Datei daher auf eine solche ablegen. Damit können wir auch eventuelle Einschränkungen für den Dateizugriff umgehen\footnote{Unter Android laufen alle Applikationen mit einer individuellen Benutzer- und Prozess-Identifikation und das System erlaubt keinen Zugriff auf die Dateien anderer Applikationen.}.

Das eigentliche Laden des Graphen erfolgt in der \emph{initialize()} Methode der Strategie und verwendet das \emph{IGraphFactory} Interface. Dieses definiert eine Schnittstelle zur Erzeugung des Wurzel-Knoten eines Graphen nach dem Factory Method Design Pattern\cite[S.107-116]{designpatterns} und wird von der Klasse \emph{GraphFactoryGraphMLImport} implementiert. Diese delegiert das Erzeugen wiederum an das \emph{IGraphMLParser} Interface und dessen konkrete Implementation \emph{GraphMLPullParser}, welches einen XML Pull Parser\footnote{Im Prototyp wurde noch ein SAX-Parser verwendet, aber die Android API stellt zum Einlesen von XML-Dateien aus den Applikations-Ressourcen einen Pull Parser zur Verfügung, weshalb schlussendlich dieser verwendet wurde.} zum Einlesen der GraphML-Datei verwenden. \emph{GraphMLPullParser} versucht dabei zuerst, sofern der Parsing-Methode ein Pfad übergeben wurde, die entsprechende XML-Datei von der SD-Karte einzulesen. Ist diese nicht vorhanden, wird die im Applikations-Paket mitgelieferte GraphML-Datei eingelesen.

\begin{lstlisting} [caption={Initialisierung von \emph{CharacterDetectionStrategyGraph}},label=cds_graph_init]
public void initialize() {
  if (!hasArgument("Path")) {
    addArgument(new StrategyArgument(
      getStrategyName(), 
      "Path", 
      "/sdcard/ba10_bsha_1.xml", 
      "Path_of_the_GraphML_file"));
  }

  IGraphFactory factory = new GraphFactoryGraphMLImport(
    getArgument("Path").getArgumentValue());
  root = factory.createRoot();	
}
\end{lstlisting}

Durch dieses Vorgehen muss der Graph nur beim ersten Aufstarten des Service geparsed und erzeugt werden und kann dann im Speicher gehalten werden. So lange ein Frontend an unseren \emph{DetectionService} gebunden ist, sollte dieser vom Betriebssystem nur im Falle eines äusserst gravierenden Ressourcen-Mangels terminiert werden. Da unser Frontend einen \emph{InputMethodService} implementieren wird und somit auch wenn der Benutzer keine Eingaben tätigt von Android im Hintergrund bereit gehalten werden sollte, sollte dies beinahe nie der Fall sein. Sollte es trotzdem einmal erwünscht sein, den Graphen neu oder von einer anderen Datei einzulesen, kann über die Schnittstellen\footnote{Über die Methode \emph{reinitialize()} mit der Typen-Nummer der Zeichen-Erkennungsstrategie als Parameter. Soll eine andere Datei verwendet werden, muss diese zuerst mit über das entsprechende \emph{StrategyArgument} spezifiziert werden.} des \emph{DetectionService} eine Reinitialisierung der Zeichen-Erkennungs-Strategie veranlasst werden. Dabei wird auch der Graph neu aufgebaut.

Werden dann später die erkannten Microgesten der Zeichen-Erkennung zugeführt, reicht die \emph{detectCharater()} Methode von \emph{CharacterDetectionStrategyGraph} diese an den Wurzel-Knoten des Graphen weiter. Dessen Methode \emph{consume()} nimmt diese entgegen, führt die Erkennung durch und gibt die gefundenen Zeichen zurück.

\begin{figure}[h]
   \centering
   \includegraphics[width=\textwidth]{img/uml_sd_consume} 
   \caption{Grober Ablauf der Erkennung in \emph{Node}}
   \label{fig:sd_consume}
\end{figure}

\begin{lstlisting} [caption={Methode zur Zeichen-Erkennung in \emph{Node}},label=node_consume]
public Collection<Character> consume(
    Collection<MicroGesture> micro_gestures, 
    float probability) {
  ArrayList<Character> result;
  result = new ArrayList<Character>();
  //Weiterfahren wenn noch MicroGestures zum 
  //Konsum vorhanden sind
  if ((micro_gestures != null) 
      && (micro_gestures.size() > 0) 
      && (probability > MIN_PROBABILITY)) {
    //Testen, ob dem Node ein MicroGestureTester 
    //zugewiesen ist
    if (tester != null) {
      MicroGesture mg = micro_gestures.iterator().next();
      if (tester.validate(mg)) {
        //Positiv evaluierte MicroGestures aus den noch 
        //zu verfuetternden heraus nehmen
        micro_gestures.remove(mg);
        //Character des Nodes den Resultaten hinzufuegen
        result.add(new Character(
          null, character, probability));
        //Uebrige MicroGestures an die folgenden 
        //Knoten verfuettern
        for (Edge edge : outgoingEdges) {
          result.addAll(
            edge.getDestination().consume(
              micro_gestures, 
              (edge.getProbability() * probability)));
        }
      }
    //Alle MicroGestures an die folgenden Knoten ver-
    //fuettern wenn kein MicroGestureTester vorhanden
    } else {
      for (Edge edge : outgoingEdges) {
        result.addAll(
          edge.getDestination().consume(
            micro_gestures, 
            (edge.getProbability() * probability)));
      }
    }
  }
  //Resultate nach Wahrscheinlichkeit ordnen
  Collections.sort(result);
  return result;
}
\end{lstlisting}


\subsubsection{Beispiel einer neuen Strategie}

Zur Veranschaulichung wollen wir nun kurz demonstrieren, wie eine neue Strategie in unser System hinzugefügt werden könnte. Als Beispiel wollen wir jeweils eine Strategy implementieren, welche die übergebenen Eingabe-Punkte beziehungsweise die übergebenen Mikrogesten über das Logcat Logging-Interface ausgibt. Diese könnten etwa als ersten und letzten Schritt der Vorverarbeitung eingebunden werden um deren Effekte zu beobachten, während die Mikrogesten-Erkennungsstrategie die erkannten Mikrogesten ausgeben würde.


\subsection{Service}\label{lbl_be_impl_service}

%================================================

